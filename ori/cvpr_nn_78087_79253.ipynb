{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cplfw_rank': 9, 'market1501_rank': 361, 'dukemtmc_rank': 426, 'msmt17_rank': 433, 'veri_rank': 327, 'vehicleid_rank': 480, 'veriwild_rank': 425, 'sop_rank': 367, 'arch': 'l231131331121121331111211121331321321'}\n",
      "train_num: 500\n"
     ]
    }
   ],
   "source": [
    "# 读取训练数据, 训练集包含500个模型结构，以及这些结构在cplfw，market1501，dukemtmc等8个任务上的性能排序\n",
    "import json\n",
    "with open('./data/CVPR_2022_NAS_Track2_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data['arch1'])\n",
    "print('train_num:',len(train_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['cplfw_rank', 'market1501_rank', 'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank', 'veriwild_rank', 'sop_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(train_data):\n",
    "    ret = []\n",
    "    for k, v in train_data.items():\n",
    "        tmp = list(v['arch'])\n",
    "        tmp1 = []\n",
    "        for c in target_cols:\n",
    "            tmp1.append(v[c])\n",
    "        ret.append(tmp+tmp1+[k,v['arch']])\n",
    "    retf = pd.DataFrame(ret,columns=[f'col{_}' for _ in range(len(tmp))]+target_cols+['id','arch'])\n",
    "    retf['col0'] = retf['col0'].map({'l':1, 'j':2, 'k':3})\n",
    "    int_cols = [x for x in retf.columns if x not in ['id','arch']]\n",
    "    retf[int_cols] = retf[int_cols].astype(float)\n",
    "    return retf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_df(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfinv \n",
    "\n",
    "for c in target_cols:\n",
    "    train_df[c+'_log1p'] = train_df[c].map(lambda x: np.log1p(x))\n",
    "    train_y=train_df[c]\n",
    "    mmin=np.min(train_y)+1\n",
    "    mmax=np.max(train_y)+1\n",
    "    train_y=np.sqrt(2) * erfinv(2 * (train_y+mmin)/(mmin+mmax)-1)\n",
    "    train_df[c+'_trans_y'] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8',\n",
       "       'col9', 'col10', 'col11', 'col12', 'col13', 'col14', 'col15', 'col16',\n",
       "       'col17', 'col18', 'col19', 'col20', 'col21', 'col22', 'col23', 'col24',\n",
       "       'col25', 'col26', 'col27', 'col28', 'col29', 'col30', 'col31', 'col32',\n",
       "       'col33', 'col34', 'col35', 'col36', 'cplfw_rank', 'market1501_rank',\n",
       "       'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank',\n",
       "       'veriwild_rank', 'sop_rank', 'id', 'arch', 'cplfw_rank_log1p',\n",
       "       'cplfw_rank_trans_y', 'market1501_rank_log1p',\n",
       "       'market1501_rank_trans_y', 'dukemtmc_rank_log1p',\n",
       "       'dukemtmc_rank_trans_y', 'msmt17_rank_log1p', 'msmt17_rank_trans_y',\n",
       "       'veri_rank_log1p', 'veri_rank_trans_y', 'vehicleid_rank_log1p',\n",
       "       'vehicleid_rank_trans_y', 'veriwild_rank_log1p',\n",
       "       'veriwild_rank_trans_y', 'sop_rank_log1p', 'sop_rank_trans_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>...</th>\n",
       "      <th>msmt17_rank_log1p</th>\n",
       "      <th>msmt17_rank_trans_y</th>\n",
       "      <th>veri_rank_log1p</th>\n",
       "      <th>veri_rank_trans_y</th>\n",
       "      <th>vehicleid_rank_log1p</th>\n",
       "      <th>vehicleid_rank_trans_y</th>\n",
       "      <th>veriwild_rank_log1p</th>\n",
       "      <th>veriwild_rank_trans_y</th>\n",
       "      <th>sop_rank_log1p</th>\n",
       "      <th>sop_rank_trans_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.073045</td>\n",
       "      <td>1.108919</td>\n",
       "      <td>5.793014</td>\n",
       "      <td>0.398015</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>1.751613</td>\n",
       "      <td>6.054439</td>\n",
       "      <td>1.037718</td>\n",
       "      <td>5.908083</td>\n",
       "      <td>0.626575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.192362</td>\n",
       "      <td>1.978217</td>\n",
       "      <td>5.966147</td>\n",
       "      <td>0.766946</td>\n",
       "      <td>6.030685</td>\n",
       "      <td>0.955507</td>\n",
       "      <td>6.011267</td>\n",
       "      <td>0.894120</td>\n",
       "      <td>5.713733</td>\n",
       "      <td>0.265766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.411646</td>\n",
       "      <td>-0.132977</td>\n",
       "      <td>4.990433</td>\n",
       "      <td>-0.543441</td>\n",
       "      <td>5.198497</td>\n",
       "      <td>-0.355046</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>-0.835931</td>\n",
       "      <td>3.555348</td>\n",
       "      <td>-1.476832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   col0  col1  col2  col3  col4  col5  col6  col7  col8  col9  ...  \\\n",
       "0   1.0   2.0   3.0   1.0   1.0   3.0   1.0   3.0   3.0   1.0  ...   \n",
       "1   3.0   1.0   1.0   1.0   2.0   2.0   1.0   3.0   2.0   1.0  ...   \n",
       "2   2.0   2.0   2.0   1.0   3.0   1.0   1.0   2.0   1.0   1.0  ...   \n",
       "\n",
       "   msmt17_rank_log1p  msmt17_rank_trans_y  veri_rank_log1p  veri_rank_trans_y  \\\n",
       "0           6.073045             1.108919         5.793014           0.398015   \n",
       "1           6.192362             1.978217         5.966147           0.766946   \n",
       "2           5.411646            -0.132977         4.990433          -0.543441   \n",
       "\n",
       "   vehicleid_rank_log1p  vehicleid_rank_trans_y  veriwild_rank_log1p  \\\n",
       "0              6.175867                1.751613             6.054439   \n",
       "1              6.030685                0.955507             6.011267   \n",
       "2              5.198497               -0.355046             4.615121   \n",
       "\n",
       "   veriwild_rank_trans_y  sop_rank_log1p  sop_rank_trans_y  \n",
       "0               1.037718        5.908083          0.626575  \n",
       "1               0.894120        5.713733          0.265766  \n",
       "2              -0.835931        3.555348         -1.476832  \n",
       "\n",
       "[3 rows x 63 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(train_df):\n",
    "    res0 = []\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    res3 = []\n",
    "    res4 = []\n",
    "    map_={'l':1, 'j':2, 'k':3}\n",
    "    time_step=12\n",
    "    for item in train_df.arch:\n",
    "        ret = np.array(list(item[1:]),dtype=np.float32).reshape(-1,3)\n",
    "        res0.append(ret[:,0][:time_step])\n",
    "        res1.append(ret[:,1][:time_step])\n",
    "        res2.append(ret[:,2][:time_step])\n",
    "        res3.append(np.array([map_[item[0]]]*time_step))\n",
    "        res4.append([map_[item[0]]]+list(np.array(list(item[1:]),dtype=np.float32)))\n",
    "        \n",
    "    train_df['head'] = res0\n",
    "    train_df['mlp'] = res1\n",
    "    train_df['emb'] = res2\n",
    "    train_df['depth'] = res3\n",
    "    train_df['all_emb'] = res4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     retf['col0'] = retf['col0'].map({'j':10, 'k':11, 'l':12})\n",
    "#     retf[num_heads] = retf[num_heads].replace({1:12, 2:11, 3:10})\n",
    "#     retf[mlp_ratio] = retf[mlp_ratio].replace({1:4, 2:3.5, 3:3})\n",
    "#     retf[emb_dim] = retf[emb_dim].replace({1:768})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(train_df):\n",
    "    res0 = []\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    res3 = []\n",
    "    res4 = []\n",
    "    map_={'l':1, 'j':2, 'k':3}\n",
    "    time_step=12\n",
    "    head_ = set([1+_*3 for _ in range(12)])\n",
    "    mlp_ = set([2+_*3 for _ in range(12)])\n",
    "    emb_ = set([3+_*3 for _ in range(12)])\n",
    "    depth_ = set([0])\n",
    "    for item in train_df.arch:\n",
    "        ret = np.array(list(item[1:]),dtype=np.float32).reshape(-1,3)\n",
    "        res0.append([1 if x in head_ else 0 for x in range(37)])\n",
    "        res1.append([1 if x in mlp_ else 0 for x in range(37)])\n",
    "        res2.append([1 if x in emb_ else 0 for x in range(37)])\n",
    "        res3.append([1 if x in depth_ else 0 for x in range(37)])\n",
    "        res4.append([map_[item[0]]]+list(np.array(list(item[1:]),dtype=np.float32)))\n",
    "        \n",
    "    train_df['head'] = res0\n",
    "    train_df['mlp'] = res1\n",
    "    train_df['emb'] = res2\n",
    "    train_df['depth'] = res3\n",
    "    train_df['all_emb'] = res4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_step(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_0503(train_df):\n",
    "    res0 = []\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    res3 = []\n",
    "    res4 = []\n",
    "    map_={'l':1, 'j':2, 'k':3}\n",
    "    time_step=12\n",
    "    head_ = set([1+_*3 for _ in range(12)])\n",
    "    mlp_ = set([2+_*3 for _ in range(12)])\n",
    "    emb_ = set([3+_*3 for _ in range(12)])\n",
    "    depth_ = set([0])\n",
    "    for item in train_df.arch:\n",
    "        ret = np.array(list(item[1:]),dtype=np.float32).reshape(-1,3)\n",
    "        res0.append([int(item[x]) if x in head_ else 0 for x in range(37)])\n",
    "        res1.append([int(item[x]) if x in mlp_ else 0 for x in range(37)])\n",
    "        res2.append([int(item[x]) if x in emb_ else 0 for x in range(37)])\n",
    "        res3.append([map_[item[0]]]+[0]*36)\n",
    "        res4.append([map_[item[0]]]+list(np.array(list(item[1:]),dtype=np.float32)))\n",
    "        \n",
    "    train_df['head'] = res0\n",
    "    train_df['mlp'] = res1\n",
    "    train_df['emb'] = res2\n",
    "    train_df['depth'] = res3\n",
    "    train_df['all_emb'] = res4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base cols len: 37\n",
      "embedding size: 93\n"
     ]
    }
   ],
   "source": [
    "#获取col级别的ohe映射\n",
    "base_cols = [x for x in train_df.columns if x[:3]=='col']\n",
    "print(f'base cols len: {len(base_cols)}')\n",
    "cnt=0\n",
    "emb_map={}\n",
    "for i, c in enumerate(base_cols):\n",
    "    nunique = sorted(train_df[c].unique())\n",
    "    emb_map[i] = {}\n",
    "    for v in nunique:\n",
    "        emb_map[i][v]=cnt\n",
    "        cnt+=1\n",
    "emb_size = 0\n",
    "for k, v in emb_map.items():\n",
    "    emb_size += len(v)\n",
    "print(f'embedding size: {emb_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_emb_trans(x, emb_map):\n",
    "    ret=[]\n",
    "    for i, v in enumerate(x):\n",
    "        ret.append(emb_map[i][v])\n",
    "    return ret\n",
    "    \n",
    "def ohe_trans(train_df, emb_map):\n",
    "    train_df['all_emb_vec'] = train_df['all_emb'].map(lambda x: all_emb_trans(x, emb_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_trans(train_df, emb_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  8, ..., 86, 89, 92],\n",
       "       [ 2,  3,  6, ..., 83, 87, 91],\n",
       "       [ 1,  4,  7, ..., 83, 87, 91],\n",
       "       ...,\n",
       "       [ 0,  3,  6, ..., 85, 90, 92],\n",
       "       [ 2,  3,  7, ..., 83, 87, 91],\n",
       "       [ 0,  5,  7, ..., 86, 88, 92]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_df['all_emb_vec'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义数据\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,df,use_cols,target_cols,show=0):\n",
    "        self.df = df\n",
    "        self.show = show\n",
    "        self.use_cols = use_cols\n",
    "        self.target_cols = target_cols\n",
    "\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def prepare_data(self):\n",
    "#         self.y = self.df[self.target_cols].values\n",
    "        self.y = self.df[[x+'_trans_y' for x in self.target_cols]].values\n",
    "        self.y0 = self.df[[self.target_cols[0]]].values\n",
    "        self.y1 = self.df[[self.target_cols[1]]].values\n",
    "        self.y2 = self.df[[self.target_cols[2]]].values\n",
    "        self.y3 = self.df[[self.target_cols[3]]].values\n",
    "        self.y4 = self.df[[self.target_cols[4]]].values\n",
    "        self.y5 = self.df[[self.target_cols[5]]].values\n",
    "        self.y6 = self.df[[self.target_cols[6]]].values\n",
    "        self.y7 = self.df[[self.target_cols[7]]].values\n",
    "        \n",
    "        self.inputs = np.array(self.df['all_emb_vec'].tolist())\n",
    "        \n",
    "#         uc = ['all_emb_vec']\n",
    "#         tmp_dt = {}\n",
    "#         for c in uc:\n",
    "#             tmp_dt[c] = np.array(self.df[c].tolist())\n",
    "#         self.inputs = np.concatenate([tmp_dt[c][:, None] for c in uc], 1).transpose(0, 2, 1)\n",
    "\n",
    "\n",
    "#         uc= ['all_emb','depth','head','mlp','emb']\n",
    "#         tmp_dt = {}\n",
    "#         for c in uc:\n",
    "#             tmp_dt[c] = np.array(self.df[c].tolist())\n",
    "#         self.inputs1= np.concatenate([tmp_dt[c][:, None] for c in uc], 1).transpose(0, 2, 1)\n",
    "        \n",
    "        \n",
    "        if self.show==1:\n",
    "            print('inputs_shape',self.inputs.shape)\n",
    "#             print('inputs1_shape',self.inputs1.shape)\n",
    "            print('y_shape',self.y.shape)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = {\n",
    "            \"input\": F.one_hot(torch.tensor(self.inputs[idx]).long(), num_classes=emb_size).float(),\n",
    "#             \"input1\": torch.tensor(self.inputs1[idx], dtype=torch.float),\n",
    "            \"y\": torch.tensor(self.y[idx], dtype=torch.float),\n",
    "#             \"y_trans\": torch.tensor(self.y[idx], dtype=torch.float),\n",
    "            \"y0\": torch.tensor(self.y0[idx], dtype=torch.float),\n",
    "            \"y1\": torch.tensor(self.y1[idx], dtype=torch.float),\n",
    "            \"y2\": torch.tensor(self.y2[idx], dtype=torch.float),\n",
    "            \"y3\": torch.tensor(self.y3[idx], dtype=torch.float),\n",
    "            \"y4\": torch.tensor(self.y4[idx], dtype=torch.float),\n",
    "            \"y5\": torch.tensor(self.y5[idx], dtype=torch.float),\n",
    "            \"y6\": torch.tensor(self.y6[idx], dtype=torch.float),\n",
    "            \"y7\": torch.tensor(self.y7[idx], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['col0', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8',\n",
       "       'col9', 'col10', 'col11', 'col12', 'col13', 'col14', 'col15', 'col16',\n",
       "       'col17', 'col18', 'col19', 'col20', 'col21', 'col22', 'col23', 'col24',\n",
       "       'col25', 'col26', 'col27', 'col28', 'col29', 'col30', 'col31', 'col32',\n",
       "       'col33', 'col34', 'col35', 'col36', 'cplfw_rank', 'market1501_rank',\n",
       "       'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank',\n",
       "       'veriwild_rank', 'sop_rank', 'id', 'arch', 'cplfw_rank_log1p',\n",
       "       'cplfw_rank_trans_y', 'market1501_rank_log1p',\n",
       "       'market1501_rank_trans_y', 'dukemtmc_rank_log1p',\n",
       "       'dukemtmc_rank_trans_y', 'msmt17_rank_log1p', 'msmt17_rank_trans_y',\n",
       "       'veri_rank_log1p', 'veri_rank_trans_y', 'vehicleid_rank_log1p',\n",
       "       'vehicleid_rank_trans_y', 'veriwild_rank_log1p',\n",
       "       'veriwild_rank_trans_y', 'sop_rank_log1p', 'sop_rank_trans_y', 'head',\n",
       "       'mlp', 'emb', 'depth', 'all_emb', 'all_emb_vec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = train_df.sample(400, random_state=666).reset_index(drop=True)\n",
    "val0 = train_df[~train_df.id.isin(train0.id)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [x for x in train_df.columns if 'col' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_shape (400, 37)\n",
      "y_shape (400, 8)\n",
      "inputs_shape (100, 37)\n",
      "y_shape (100, 8)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(train0, use_cols, target_cols, 1)\n",
    "val_dataset = MyDataset(val0, use_cols, target_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:3]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "def count_parameters(model, all=False):\n",
    "    if all:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_model_weights(model, modelpath, filename):\n",
    "    torch.save(model.state_dict(), modelpath+filename)\n",
    "    return f\"\\n -> Save weights to {modelpath+filename}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(pred, y):\n",
    "    corr = []\n",
    "    if pred.shape[1]>2:\n",
    "        for i in range(8):\n",
    "            corr.append(scipy.stats.stats.kendalltau(pred[:, i], y[:, i])[0])\n",
    "    else:\n",
    "        corr.append(scipy.stats.stats.kendalltau(pred, y)[0])\n",
    "    return np.array(corr)\n",
    "\n",
    "class CVPRLoss(nn.Module):\n",
    "    def __call__(self, pred, y):\n",
    "        pred_mean, y_mean = pred.mean(0), y.mean(0)\n",
    "        pred_std, y_std = pred.std(0), y.std(0)\n",
    "        corr = ((pred - pred_mean) * (y - y_mean)).sum(0) / ((((pred - pred_mean)**2).sum(0).sqrt()) * (((y - y_mean)**2).sum(0).sqrt())) \n",
    "    \n",
    "#         cos = nn.CosineSimilarity(dim=0, eps=1e-08)\n",
    "#         corr1 = cos(pred, y)\n",
    "#         corr2 = (corr+corr1)/2\n",
    "        return 1-corr\n",
    "\n",
    "from fast_soft_sort.pytorch_ops import soft_rank\n",
    "class CVPRLoss1(nn.Module):\n",
    "    #spearman\n",
    "    def __call__(self, pred, y):\n",
    "        pred = pred.cpu()\n",
    "        y = y.cpu()\n",
    "        return 1-torch.cat(\n",
    "            [self.spearman(y[:,i].reshape(1,-1), \n",
    "                           pred[:,i].reshape(1,-1)).reshape(1,1) for i in range(y.shape[1])]\n",
    "        ).reshape(1, -1)\n",
    "        \n",
    "    def corrcoef(self, target, pred):\n",
    "        # np.corrcoef in torch from @mdo\n",
    "        # https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960\n",
    "        pred_n = pred - pred.mean()\n",
    "        target_n = target - target.mean()\n",
    "        pred_n = pred_n / pred_n.norm()\n",
    "        target_n = target_n / target_n.norm()\n",
    "        return (pred_n * target_n).sum()\n",
    "\n",
    "\n",
    "    def spearman(self,\n",
    "        target,\n",
    "        pred,\n",
    "        regularization=\"l2\",\n",
    "        regularization_strength=0.01,\n",
    "    ):\n",
    "        # fast_soft_sort uses 1-based indexing, divide by len to compute percentage of rank\n",
    "        pred = soft_rank(\n",
    "            pred,\n",
    "            regularization=regularization,\n",
    "            regularization_strength=regularization_strength,\n",
    "        )\n",
    "    #     print(pred)\n",
    "        return self.corrcoef(target, pred / pred.shape[-1])\n",
    "     \n",
    "class CVPRLoss2(nn.Module):\n",
    "    #kendall soft rank+tanh\n",
    "    def __call__(self, pred, y):\n",
    "        pred = pred.cpu()\n",
    "        y = y.cpu()\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(y[:,i].reshape(1,-1), \n",
    "                           pred[:,i].reshape(1,-1)).reshape(1,-1) for i in range(y.shape[1])]\n",
    "        ).reshape(1, -1)\n",
    "        \n",
    "    def get_score(self, actuals, preds):\n",
    "        sa = actuals.argsort()[0]\n",
    "\n",
    "        tmp = soft_rank(\n",
    "            preds.index_select(1, sa),\n",
    "            regularization='l2',\n",
    "            regularization_strength=0.01,\n",
    "        )[0]-1\n",
    "        \n",
    "        tmp = tmp.cuda()\n",
    "\n",
    "        score = torch.cat([((tmp[i:]-tmp[i-1]) * 10).tanh() for i in range(1, tmp.shape[0])]).sum()\n",
    "        score1 = score/sum(list(range(1,len(tmp))))\n",
    "        return score1\n",
    "    \n",
    "class CVPRLoss_softrank(nn.Module):\n",
    "    #kendall soft rank+tanh\n",
    "    def __call__(self, pred, y):\n",
    "        pred = pred.cpu()\n",
    "        y = y.cpu()\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(y[:,i].reshape(1,-1), \n",
    "                           pred[:,i].reshape(1,-1)).reshape(1,-1) for i in range(y.shape[1])]\n",
    "        ).reshape(1, -1)\n",
    "        \n",
    "    def get_score(self, actuals, preds):\n",
    "        sa = actuals.argsort()[0]\n",
    "\n",
    "        tmp = soft_rank(\n",
    "            preds.index_select(1, sa),\n",
    "            regularization='l2',\n",
    "            regularization_strength=0.01,\n",
    "        )[0]-1\n",
    "        \n",
    "        tmp = tmp.cuda()\n",
    "\n",
    "        score = torch.cat([((tmp[i:]-tmp[i-1]) * 10).tanh() for i in range(1, tmp.shape[0])]).sum()\n",
    "        score1 = score/sum(list(range(1,len(tmp))))\n",
    "        return score1\n",
    "    \n",
    "class CVPRLoss_softsign(nn.Module):\n",
    "    #kendall softsign\n",
    "    def __call__(self, pred, y):\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(y[:,i].reshape(1,-1), \n",
    "                           pred[:,i].reshape(1,-1)).reshape(1,-1) for i in range(y.shape[1])]\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "    \n",
    "    def get_score(self, actuals, preds):\n",
    "        sa = actuals.argsort()[0]\n",
    "        tmp = preds.index_select(1, sa.int())[0]\n",
    "        score = torch.cat([(tmp[i:]-tmp[i-1])/(0.01+torch.abs(tmp[i:]-tmp[i-1])) for i in range(1, tmp.shape[0])]).sum()\n",
    "        score1 = score/sum(list(range(1,len(tmp))))\n",
    "        return score1\n",
    "    \n",
    "class CVPRLoss_tanh(nn.Module):\n",
    "    # kendall tanh\n",
    "    def __call__(self, pred, y):\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(y[:,i].reshape(1,-1), \n",
    "                           pred[:,i].reshape(1,-1)).reshape(1,-1) for i in range(y.shape[1])]\n",
    "        ).reshape(1, -1)\n",
    "    \n",
    "    def get_score(self, actuals, preds):\n",
    "        sa = actuals.argsort()[0]\n",
    "        tmp = preds.index_select(1, sa.int())[0]\n",
    "        score = torch.cat([((tmp[i:]-tmp[i-1])).tanh() for i in range(1, tmp.shape[0])]).sum()\n",
    "        score1 = score/sum(list(range(1,len(tmp))))\n",
    "        return score1\n",
    "    \n",
    "    \n",
    "class CVPRLoss_pair(nn.Module):\n",
    "    def __call__(self, pred, y):\n",
    "        return torch.cat(\n",
    "            [self.pair_loss(pred[:,i], y[:,i]).reshape(1) for i in range(y.shape[1])]\n",
    "        ).reshape(1,-1)\n",
    "    \n",
    "    def pair_loss(self, outputs, labels):\n",
    "        output = outputs.unsqueeze(1)\n",
    "        output1 = output.repeat(1,outputs.shape[0])\n",
    "        label = labels.unsqueeze(1)\n",
    "        label1 = label.repeat(1,labels.shape[0])\n",
    "\n",
    "        tmp = (output1-output1.t())*torch.sign(label1-label1.t())\n",
    "        tmp = torch.log(1+torch.exp(-tmp))\n",
    "        eye_tmp = tmp*torch.eye(len(tmp)).cuda()\n",
    "        new_tmp = tmp - eye_tmp\n",
    "        loss = torch.sum(new_tmp)/(outputs.shape[0]*(outputs.shape[0]-1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CVPRLoss_tanh1(nn.Module):\n",
    "    # kendall tanh\n",
    "    def __call__(self, pred, y):\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(pred[:,i], y[:,i]).reshape(1) for i in range(y.shape[1])]\n",
    "        ).reshape(1,-1)\n",
    "    \n",
    "    def get_score(self, outputs, labels):\n",
    "        output1 = outputs.unsqueeze(1).repeat(1,outputs.shape[0])\n",
    "        label1 = labels.unsqueeze(1).repeat(1,labels.shape[0])\n",
    "\n",
    "        tmp = ((output1-output1.t())*torch.sign(label1-label1.t())).tanh()\n",
    "        eye_tmp = tmp*torch.eye(tmp.shape[0]).cuda()\n",
    "        new_tmp = tmp - eye_tmp\n",
    "        \n",
    "        loss = torch.sum(new_tmp)/(outputs.shape[0]*(outputs.shape[0]-1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10,8)\n",
    "b = torch.rand(10,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2938, 0.2423, 0.7752, 0.0256, 0.1991, 0.9268, 0.0134, 0.4208],\n",
       "        [0.6928, 0.4624, 0.2987, 0.3412, 0.1029, 0.0951, 0.0813, 0.6073],\n",
       "        [0.7384, 0.9386, 0.9592, 0.0099, 0.2762, 0.9415, 0.9735, 0.2123],\n",
       "        [0.4335, 0.7689, 0.7141, 0.6306, 0.9647, 0.6990, 0.2901, 0.8998],\n",
       "        [0.7991, 0.8953, 0.9382, 0.5447, 0.4371, 0.3246, 0.6299, 0.1848],\n",
       "        [0.8725, 0.5740, 0.6805, 0.2139, 0.8551, 0.4425, 0.3220, 0.5634],\n",
       "        [0.9424, 0.1038, 0.3128, 0.9155, 0.1701, 0.4086, 0.1180, 0.0268],\n",
       "        [0.4690, 0.5625, 0.6190, 0.5634, 0.2022, 0.9743, 0.8215, 0.7969],\n",
       "        [0.3751, 0.8123, 0.4700, 0.1645, 0.0775, 0.7283, 0.9384, 0.5328],\n",
       "        [0.8556, 0.8728, 0.7289, 0.7389, 0.8056, 0.3314, 0.1437, 0.3971]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7886, 0.7386, 0.6597, 0.2135, 0.8904, 0.2576, 0.5863, 0.9053],\n",
       "        [0.9750, 0.5364, 0.0218, 0.8977, 0.9828, 0.5364, 0.4371, 0.9436],\n",
       "        [0.2283, 0.6415, 0.6987, 0.8738, 0.3110, 0.4822, 0.2738, 0.2551],\n",
       "        [0.2761, 0.3374, 0.1084, 0.9647, 0.6921, 0.8249, 0.4304, 0.2376],\n",
       "        [0.1829, 0.0712, 0.9496, 0.6148, 0.5417, 0.6939, 0.6082, 0.3026],\n",
       "        [0.2022, 0.3713, 0.7872, 0.2586, 0.4004, 0.5719, 0.5471, 0.3807],\n",
       "        [0.4728, 0.3330, 0.7842, 0.4638, 0.5625, 0.0845, 0.6487, 0.7132],\n",
       "        [0.3503, 0.6804, 0.7270, 0.3791, 0.6012, 0.0648, 0.1171, 0.0075],\n",
       "        [0.5199, 0.0925, 0.0760, 0.2101, 0.7753, 0.2312, 0.1764, 0.7262],\n",
       "        [0.7131, 0.3528, 0.6152, 0.0677, 0.3141, 0.8574, 0.3529, 0.7444]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = CVPRLoss_tanh()\n",
    "loss1 = CVPRLoss_tanh1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0851, 1.0981, 0.8938, 0.9677, 1.1583, 1.1568, 1.2418, 1.0679]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss0(a,b).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0851, 1.0981, 0.8938, 0.9677, 1.1583, 1.1568, 1.2418, 1.0679]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3669, -0.1380,  0.7941,  ..., -0.0175,  0.4473, -0.2348],\n",
       "        [-0.2091,  1.2067,  0.9634,  ...,  0.6084,  1.2492,  1.7070],\n",
       "        [-1.2067, -0.9398,  0.1836,  ...,  1.1467, -0.3872, -0.6698],\n",
       "        ...,\n",
       "        [ 0.2348, -0.9634, -1.5903,  ...,  0.5904, -0.7404, -1.6458],\n",
       "        [-0.7536,  0.1735,  1.2714,  ...,  0.9398,  0.1330, -0.1481],\n",
       "        [-0.6327, -0.6205, -0.7872,  ...,  1.9782, -0.3232, -0.4695]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        verbose=20, \n",
    "        fold_=0,\n",
    "        modelname='MLP_base',\n",
    "        modelpath=r'./model'+'//',\n",
    "        input='input',\n",
    "        y='y',\n",
    "        early_stop_round=60,\n",
    "        debug=False):\n",
    "    \n",
    "    print(f'Model parameters count: {count_parameters(model)}')\n",
    "    #数据加载\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE_TEST,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f'train batch num: {len(train_loader)}')\n",
    "    print(f'val batch num: {len(val_loader)}')\n",
    "            \n",
    "    # Optimizer\n",
    "    optimizer = getattr(torch.optim, optim)(model.parameters(), lr=LR)\n",
    "    # Scheduler\n",
    "    num_warmup_steps = int(0.1 * EPOCHS * len(train_loader))\n",
    "    num_training_steps = int(EPOCHS * len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps, num_training_steps\n",
    "    )\n",
    "    print(f'optim: {optim}, lr: {LR}, warmup_steps: {num_warmup_steps}')\n",
    "    \n",
    "    \n",
    "    #train\n",
    "    bst_epoch=0\n",
    "    score_best=0\n",
    "    first_epoch_eval=0\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch > early_stop_round and (epoch - bst_epoch > early_stop_round):\n",
    "            print(f'early stopping.')\n",
    "            break\n",
    "            \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        start_time = time.time()\n",
    "\n",
    "        avg_loss = 0\n",
    "        for data in train_loader:\n",
    "            pred = model(data[input].to(device))\n",
    "\n",
    "            loss = loss_fct(\n",
    "                pred,\n",
    "                data[y].to(device)\n",
    "            ).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        #VAL\n",
    "        model.eval()\n",
    "        mae, avg_val_loss = 0, 0\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                pred = model(data[input].to(device))\n",
    "\n",
    "                loss = loss_fct(\n",
    "                    pred,\n",
    "                    data[y].to(device)\n",
    "                ).mean()\n",
    "\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "\n",
    "                preds.append(pred.detach().cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds, 0)\n",
    "        if y=='y':\n",
    "            mae = compute_metric(preds,val_dataset.df[target_cols].values).mean()\n",
    "        else:\n",
    "            mae = compute_metric(preds,val_dataset.df[[target_cols[int(y.replace('y',''))]]].values).mean()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        if (epoch + 1) % verbose == 0:\n",
    "            elapsed_time = elapsed_time * verbose\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "    #         lr=LR\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:02d}/{ EPOCHS:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n",
    "                f\"loss={avg_loss:.4f}\",\n",
    "                end=\"\\t\",\n",
    "            )\n",
    "\n",
    "            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == EPOCHS):\n",
    "                print(f\"val_loss={avg_val_loss:.4f}\\tcorr={mae:.4f}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "                \n",
    "        #保存最优模型\n",
    "        if mae>score_best:\n",
    "            bst = save_model_weights(model, modelpath, f'{modelname}_{fold_}.pt')\n",
    "            score_best = mae\n",
    "            bst_epoch = epoch\n",
    "            if y=='y':\n",
    "                bst_list = compute_metric(preds,val_dataset.df[target_cols].values)\n",
    "            else:\n",
    "                bst_list = compute_metric(preds,val_dataset.df[[target_cols[int(y.replace('y',''))]]].values).mean()\n",
    "            bst_preds = preds\n",
    "    print(f'best score {score_best}, best epoch: {bst_epoch}, {bst} ' )\n",
    "    print(bst_list,'\\n\\n')\n",
    "    del (val_loader, train_loader, loss, data, pred)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return bst_preds, bst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVPRModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3,\n",
    "        num_classes=8,\n",
    "        time_step=12,\n",
    "        bi=True\n",
    "    ):\n",
    "        super(CVPRModel,self).__init__()\n",
    "        \n",
    "        self.bi_num = 2 if bi else 1\n",
    "        self.time_step = time_step\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.LSTM0 = nn.LSTM(256, 256, \n",
    "                    batch_first=True, \n",
    "                    bidirectional=bi, \n",
    "                    num_layers=1,\n",
    "                    dropout=0)\n",
    "                                 \n",
    "        \n",
    "        self.LSTM1 = nn.LSTM(512, 256, \n",
    "                    batch_first=True, \n",
    "                    bidirectional=bi, \n",
    "                    num_layers=1,\n",
    "                    dropout=0\n",
    "                            )    \n",
    "        \n",
    "        self.Logits0 = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear((93)*self.time_step, 256),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.Logits1 = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear((256)*self.time_step, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.Logits2 = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear((512)*self.time_step, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.Logits3 = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear((512)*self.time_step, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.MLP(x)\n",
    "        x2, (h0, c0) = self.LSTM0(x1)\n",
    "        x3, (h1, c1) = self.LSTM1(x2, (h0, c0))\n",
    "        \n",
    "        p0 = self.Logits0(x)\n",
    "        p1 = self.Logits1(x1)\n",
    "        p2 = self.Logits2(x2)\n",
    "        p3 = self.Logits3(x3)\n",
    "\n",
    "\n",
    "        c1 = torch.cat([\n",
    "            p0.unsqueeze(2), \n",
    "            p1.unsqueeze(2), \n",
    "            p2.unsqueeze(2), \n",
    "            p3.unsqueeze(2), \n",
    "                       ],-1\n",
    "        )\n",
    "        \n",
    "        pred = self.fc(c1)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设置\n",
    "seed = 666\n",
    "BATCH_SIZE = 400\n",
    "BATCH_SIZE_TEST = 128\n",
    "EPOCHS = 300\n",
    "LR = 0.001\n",
    "optim = \"Adam\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "modelpath = r'./model'+'//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(128,37,93).to(device)\n",
    "net = CVPRModel(input_dim=93,\n",
    "                num_classes=len(target_cols),\n",
    "                bi=True,\n",
    "                time_step=37\n",
    "               ).to(device)\n",
    "b = net(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.flatten(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a,b,net;\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVPRLoss_tanh1(nn.Module):\n",
    "    # kendall tanh\n",
    "    def __call__(self, pred, y):\n",
    "        return 1-torch.cat(\n",
    "            [self.get_score(pred[:,i], y[:,i]).reshape(1) for i in range(y.shape[1])]\n",
    "        ).reshape(1,-1)\n",
    "    \n",
    "    def get_score(self, outputs, labels):\n",
    "        output1 = outputs.unsqueeze(1).repeat(1,outputs.shape[0])\n",
    "        label1 = labels.unsqueeze(1).repeat(1,labels.shape[0])\n",
    "\n",
    "        tmp = ((output1-output1.t())*torch.sign(label1-label1.t())).tanh()\n",
    "        eye_tmp = tmp*torch.eye(tmp.shape[0]).cuda()\n",
    "        new_tmp = tmp - eye_tmp\n",
    "        \n",
    "        loss = torch.sum(new_tmp)/(outputs.shape[0]*(outputs.shape[0]-1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=4s \tloss=0.9480\tval_loss=0.9393\tcorr=0.5272\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=4s \tloss=0.4272\tval_loss=0.4541\tcorr=0.5595\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=4s \tloss=0.2842\tval_loss=0.3034\tcorr=0.7079\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=4s \tloss=0.2290\tval_loss=0.2573\tcorr=0.7482\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=4s \tloss=0.1961\tval_loss=0.2264\tcorr=0.7783\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=3s \tloss=0.1801\tval_loss=0.2097\tcorr=0.7942\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=3s \tloss=0.1680\tval_loss=0.2133\tcorr=0.7870\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=4s \tloss=0.1527\tval_loss=0.2296\tcorr=0.7726\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=4s \tloss=0.1325\tval_loss=0.2246\tcorr=0.7774\n",
      "early stopping.\n",
      "best score 0.7942424242424243, best epoch: 119, \n",
      " -> Save weights to E:\\cvpr_model//LSTM1_step37_base1_test__t.pt\n",
      " \n",
      "[0.3430303  0.87676768 0.88767677 0.95676768 0.88444444 0.66787879\n",
      " 0.91676768 0.82060606] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#参数设置\n",
    "seed = 666\n",
    "BATCH_SIZE = 400\n",
    "BATCH_SIZE_TEST = 128\n",
    "EPOCHS = 300\n",
    "LR = 0.001\n",
    "optim = \"Adam\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "modelpath = r'E:\\cvpr_model'+'//'\n",
    "\n",
    "# loss_fct = CVPRLoss_tanh()\n",
    "loss_fct = CVPRLoss_tanh1()\n",
    "# loss_fct = nn.MSELoss()\n",
    "index = '_t'\n",
    "seed_everything(seed)\n",
    "modelname = 'LSTM1_step37_base1_test'\n",
    "model = CVPRModel(input_dim=93,\n",
    "                num_classes=8,\n",
    "                bi=True,\n",
    "                time_step=37\n",
    "               ).to(device)\n",
    "preds,_ = train(model, \n",
    "            train_dataset, \n",
    "            val_dataset, \n",
    "            verbose=20, \n",
    "            fold_=index,\n",
    "            modelname=modelname,\n",
    "            modelpath=modelpath,\n",
    "            input='input',\n",
    "            y='y',\n",
    "            debug=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.3430303  0.87676768 0.88767677 0.95676768 0.88444444 0.66787879 0.91676768 0.82060606] \n",
    "# [0.33494949 0.87111111 0.89333333 0.96040404 0.88242424 0.65616162 0.91717172 0.81414141] \n",
    "# [0.25979798 0.84808081 0.91191919 0.9620202  0.90585859 0.68767677 0.93171717 0.78585859] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7689898989898991 [0.27191919 0.85818182 0.88080808 0.93616162 0.85575758 0.6759596 0.86949495 0.80363636] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2层自带lstm 0.7911616161616162 \n",
    "# 1层的输出作为2层的h和c 0.7920707070707071\n",
    "# 2层手写，不初始化 0.7911616161616162 \n",
    "# 3层手写，输出作为输入 0.7911111111111113 \n",
    "# 2自带*2自带，输出作为输入 0.7875757575757577 \n",
    "# 1层输出作为2层输入，最后2层输出cat 0.7894444444444446\n",
    "# 1层输出作为2层输入，最后2层输出相乘 0.7885858585858587\n",
    "# 1层的输出作为2层的h和c,加一层gru cat 0.7894949494949497 \n",
    "# 1层的输出作为2层的h和c,加一层gru*1层输出 cat 0.7886868686868689\n",
    "# 1层的输出作为2层的h和c,加2层gru cat 0.7913131313131314 \n",
    "# 1层的输出作为2层的h和c,加2层gru*1层输出 cat2层输出 0.7915151515151516 \n",
    "# 1层的输出作为2层的h和c，2层的输出transformer然后乘以2层的输出，0.7899494949494951\n",
    "# 1层的输出作为2层的h和c,Tanh 0.7893939393939395 \n",
    "# 1层的输出作为2层的h和c,relu 0.7862121212121214 \n",
    "# 1层的输出作为2层的h和c,fc出去，0.7872222222222224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# index = '_t'\n",
    "# modelname = 'LSTM1_step37_base1_test'\n",
    "# scoref = []\n",
    "# for i in range(8):\n",
    "#     seed_everything(seed)\n",
    "#     model = CVPRModel(input_dim=1,\n",
    "#                     num_classes=1,\n",
    "#                     bi=True,\n",
    "#                     time_step=37\n",
    "#                    ).to(device)\n",
    "#     preds,_ = train(model, \n",
    "#                 train_dataset, \n",
    "#                 val_dataset, \n",
    "#                 verbose=20, \n",
    "#                 fold_=index,\n",
    "#                 modelname=modelname,\n",
    "#                 modelpath=modelpath,\n",
    "#                 input='input',\n",
    "#                 y=f'y{i}',\n",
    "#                 debug=False\n",
    "#                  )\n",
    "#     scoref.append(_)\n",
    "# np.mean(scoref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.33171717 0.86222222 0.88888889 0.94222222 0.8630303  0.66909091 0.9030303  0.79757576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.arange(-50,50,0.1)\n",
    "# b = a.tanh()\n",
    "# df = pd.DataFrame()\n",
    "# df['a'] = a.detach().numpy()\n",
    "# df['b'] = a.tanh()\n",
    "# df['b0'] = (a*2).tanh()\n",
    "# df['b1'] = (a*0.1).tanh()\n",
    "# df.plot(x='a',y=['b','b0','b1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sig(model, \n",
    "        train_dataset, \n",
    "        val_dataset, \n",
    "        verbose=20, \n",
    "        fold_=0,\n",
    "        modelname='MLP_base',\n",
    "        modelpath=r'./model'+'//',\n",
    "        input='input',\n",
    "        y='y',\n",
    "        early_stop_round=60,\n",
    "        debug=False):\n",
    "    \n",
    "    print(f'Model parameters count: {count_parameters(model)}')\n",
    "    #数据加载\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE_TEST,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f'train batch num: {len(train_loader)}')\n",
    "    print(f'val batch num: {len(val_loader)}')\n",
    "            \n",
    "    # Optimizer\n",
    "    optimizer = getattr(torch.optim, optim)(model.parameters(), lr=LR)\n",
    "    # Scheduler\n",
    "    num_warmup_steps = int(0.1 * EPOCHS * len(train_loader))\n",
    "    num_training_steps = int(EPOCHS * len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps, num_training_steps\n",
    "    )\n",
    "    print(f'optim: {optim}, lr: {LR}, warmup_steps: {num_warmup_steps}')\n",
    "    \n",
    "    \n",
    "    #train\n",
    "    bst_epoch={_:0 for _ in range(8)}\n",
    "    score_best={_:0 for _ in range(8)}\n",
    "    first_epoch_eval=0\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        start_time = time.time()\n",
    "\n",
    "        avg_loss = 0\n",
    "        for data in train_loader:\n",
    "            pred = model(data[input].to(device))\n",
    "#             print(pred.shape,data['y'].shape)\n",
    "\n",
    "            loss = loss_fct(\n",
    "                pred,\n",
    "                data[y].to(device)\n",
    "            ).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        #VAL\n",
    "        model.eval()\n",
    "        mae, avg_val_loss = 0, 0\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                pred = model(data[input].to(device))\n",
    "\n",
    "                loss = loss_fct(\n",
    "                    pred,\n",
    "                    data[y].to(device)\n",
    "                ).mean()\n",
    "\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "\n",
    "                preds.append(pred.detach().cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds, 0)\n",
    "        if y=='y':\n",
    "            mae = compute_metric(preds,val_dataset.df[target_cols].values).mean()\n",
    "        else:\n",
    "            mae = compute_metric(preds,val_dataset.df[[target_cols[int(y.replace('y',''))]]].values).mean()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        if (epoch + 1) % verbose == 0:\n",
    "            elapsed_time = elapsed_time * verbose\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "    #         lr=LR\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:02d}/{ EPOCHS:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n",
    "                f\"loss={avg_loss:.4f}\",\n",
    "                end=\"\\t\",\n",
    "            )\n",
    "\n",
    "            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == EPOCHS):\n",
    "                print(f\"val_loss={avg_val_loss:.4f}\\tcorr={mae:.4f}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "                \n",
    "        #保存最优模型\n",
    "        score1 = compute_metric(preds,val_dataset.df[target_cols].values)\n",
    "        for i in range(8):\n",
    "            if score1[i] > score_best[i]:\n",
    "                bst = save_model_weights(model, modelpath, f'{modelname}_target{i}_{fold_}.pt')\n",
    "                score_best[i] = score1[i]\n",
    "                bst_epoch[i] = epoch\n",
    "#                 print(f'target{i} best score {score_best[i]}, best epoch: {bst_epoch[i]}, {bst} ' )\n",
    "                \n",
    "    bst_list = [score_best[i] for i in range(8)]\n",
    "    print(bst_list,'\\n\\n')\n",
    "    del (val_loader, train_loader, loss, data, pred)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return bst_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K折训练，同时\n",
    "seed = 666\n",
    "BATCH_SIZE = 400\n",
    "BATCH_SIZE_TEST = 128\n",
    "EPOCHS = 300\n",
    "LR = 0.001\n",
    "optim = \"Adam\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "modelpath = r'E:\\cvpr_model'+'//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "train size: 400, val size: 100\n",
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 10/300 \t lr=3.3e-04\t t=3s \tloss=0.9944\tval_loss=0.9941\tcorr=0.3516\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=3s \tloss=0.9461\tval_loss=0.9386\tcorr=0.4895\n",
      "Epoch 30/300 \t lr=1.0e-03\t t=2s \tloss=0.5329\tval_loss=0.5487\tcorr=0.4619\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=3s \tloss=0.4277\tval_loss=0.4784\tcorr=0.5292\n",
      "Epoch 50/300 \t lr=9.3e-04\t t=2s \tloss=0.3558\tval_loss=0.3806\tcorr=0.6351\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=2s \tloss=0.2947\tval_loss=0.3149\tcorr=0.6967\n",
      "Epoch 70/300 \t lr=8.5e-04\t t=3s \tloss=0.2561\tval_loss=0.2833\tcorr=0.7261\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=2s \tloss=0.2305\tval_loss=0.2556\tcorr=0.7525\n",
      "Epoch 90/300 \t lr=7.8e-04\t t=2s \tloss=0.2083\tval_loss=0.2341\tcorr=0.7709\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=2s \tloss=0.1941\tval_loss=0.2224\tcorr=0.7822\n",
      "Epoch 110/300 \t lr=7.0e-04\t t=2s \tloss=0.1848\tval_loss=0.2206\tcorr=0.7833\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=2s \tloss=0.1777\tval_loss=0.2184\tcorr=0.7830\n",
      "Epoch 130/300 \t lr=6.3e-04\t t=2s \tloss=0.1741\tval_loss=0.2145\tcorr=0.7871\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=2s \tloss=0.1639\tval_loss=0.2146\tcorr=0.7875\n",
      "Epoch 150/300 \t lr=5.6e-04\t t=2s \tloss=0.1564\tval_loss=0.2187\tcorr=0.7824\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=2s \tloss=0.1814\tval_loss=0.2301\tcorr=0.7718\n",
      "Epoch 170/300 \t lr=4.8e-04\t t=2s \tloss=0.1588\tval_loss=0.2155\tcorr=0.7875\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=2s \tloss=0.1469\tval_loss=0.2161\tcorr=0.7862\n",
      "Epoch 190/300 \t lr=4.1e-04\t t=2s \tloss=0.1372\tval_loss=0.2196\tcorr=0.7830\n",
      "Epoch 200/300 \t lr=3.7e-04\t t=2s \tloss=0.1280\tval_loss=0.2221\tcorr=0.7806\n",
      "Epoch 210/300 \t lr=3.3e-04\t t=2s \tloss=0.1210\tval_loss=0.2234\tcorr=0.7787\n",
      "Epoch 220/300 \t lr=3.0e-04\t t=2s \tloss=0.1135\tval_loss=0.2237\tcorr=0.7780\n",
      "Epoch 230/300 \t lr=2.6e-04\t t=2s \tloss=0.1067\tval_loss=0.2262\tcorr=0.7753\n",
      "Epoch 240/300 \t lr=2.2e-04\t t=2s \tloss=0.1007\tval_loss=0.2269\tcorr=0.7740\n",
      "Epoch 250/300 \t lr=1.9e-04\t t=2s \tloss=0.0956\tval_loss=0.2279\tcorr=0.7732\n",
      "Epoch 260/300 \t lr=1.5e-04\t t=2s \tloss=0.0911\tval_loss=0.2302\tcorr=0.7716\n",
      "Epoch 270/300 \t lr=1.1e-04\t t=2s \tloss=0.0873\tval_loss=0.2306\tcorr=0.7707\n",
      "Epoch 280/300 \t lr=7.4e-05\t t=2s \tloss=0.0844\tval_loss=0.2311\tcorr=0.7706\n",
      "Epoch 290/300 \t lr=3.7e-05\t t=2s \tloss=0.0826\tval_loss=0.2315\tcorr=0.7697\n",
      "Epoch 300/300 \t lr=0.0e+00\t t=2s \tloss=0.0819\tval_loss=0.2315\tcorr=0.7703\n",
      "[0.32363636363636367, 0.9014141414141416, 0.9022222222222224, 0.953939393939394, 0.8892929292929294, 0.7111111111111112, 0.9244444444444447, 0.8113131313131314] \n",
      "\n",
      "\n",
      "FOLD 1\n",
      "train size: 400, val size: 100\n",
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 10/300 \t lr=3.3e-04\t t=2s \tloss=0.9945\tval_loss=0.9940\tcorr=0.3732\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=2s \tloss=0.9485\tval_loss=0.9404\tcorr=0.5469\n",
      "Epoch 30/300 \t lr=1.0e-03\t t=1s \tloss=0.5516\tval_loss=0.5328\tcorr=0.4999\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=2s \tloss=0.4299\tval_loss=0.4273\tcorr=0.5822\n",
      "Epoch 50/300 \t lr=9.3e-04\t t=2s \tloss=0.3370\tval_loss=0.3582\tcorr=0.6564\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=2s \tloss=0.2892\tval_loss=0.3235\tcorr=0.6866\n",
      "Epoch 70/300 \t lr=8.5e-04\t t=2s \tloss=0.2560\tval_loss=0.3008\tcorr=0.7065\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=3s \tloss=0.2307\tval_loss=0.2750\tcorr=0.7329\n",
      "Epoch 90/300 \t lr=7.8e-04\t t=2s \tloss=0.2119\tval_loss=0.2575\tcorr=0.7483\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=3s \tloss=0.1986\tval_loss=0.2472\tcorr=0.7573\n",
      "Epoch 110/300 \t lr=7.0e-04\t t=3s \tloss=0.1891\tval_loss=0.2368\tcorr=0.7671\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=2s \tloss=0.1819\tval_loss=0.2315\tcorr=0.7715\n",
      "Epoch 130/300 \t lr=6.3e-04\t t=2s \tloss=0.1764\tval_loss=0.2300\tcorr=0.7721\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=2s \tloss=0.1714\tval_loss=0.2273\tcorr=0.7741\n",
      "Epoch 150/300 \t lr=5.6e-04\t t=2s \tloss=0.1660\tval_loss=0.2249\tcorr=0.7782\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=2s \tloss=0.1610\tval_loss=0.2256\tcorr=0.7748\n",
      "Epoch 170/300 \t lr=4.8e-04\t t=2s \tloss=0.1527\tval_loss=0.2243\tcorr=0.7779\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=1s \tloss=0.1438\tval_loss=0.2238\tcorr=0.7778\n",
      "Epoch 190/300 \t lr=4.1e-04\t t=2s \tloss=0.1358\tval_loss=0.2240\tcorr=0.7792\n",
      "Epoch 200/300 \t lr=3.7e-04\t t=2s \tloss=0.1265\tval_loss=0.2274\tcorr=0.7747\n",
      "Epoch 210/300 \t lr=3.3e-04\t t=2s \tloss=0.1198\tval_loss=0.2315\tcorr=0.7704\n",
      "Epoch 220/300 \t lr=3.0e-04\t t=2s \tloss=0.1129\tval_loss=0.2294\tcorr=0.7720\n",
      "Epoch 230/300 \t lr=2.6e-04\t t=2s \tloss=0.1076\tval_loss=0.2302\tcorr=0.7718\n",
      "Epoch 240/300 \t lr=2.2e-04\t t=2s \tloss=0.1013\tval_loss=0.2321\tcorr=0.7682\n",
      "Epoch 250/300 \t lr=1.9e-04\t t=2s \tloss=0.0979\tval_loss=0.2336\tcorr=0.7674\n",
      "Epoch 260/300 \t lr=1.5e-04\t t=2s \tloss=0.0933\tval_loss=0.2332\tcorr=0.7683\n",
      "Epoch 270/300 \t lr=1.1e-04\t t=2s \tloss=0.0900\tval_loss=0.2344\tcorr=0.7666\n",
      "Epoch 280/300 \t lr=7.4e-05\t t=2s \tloss=0.0875\tval_loss=0.2354\tcorr=0.7663\n",
      "Epoch 290/300 \t lr=3.7e-05\t t=2s \tloss=0.0859\tval_loss=0.2362\tcorr=0.7654\n",
      "Epoch 300/300 \t lr=0.0e+00\t t=2s \tloss=0.0852\tval_loss=0.2364\tcorr=0.7646\n",
      "[0.30464646464646467, 0.8715151515151516, 0.9086868686868688, 0.9604040404040406, 0.8929292929292931, 0.6569696969696971, 0.9046464646464647, 0.7967676767676769] \n",
      "\n",
      "\n",
      "FOLD 2\n",
      "train size: 400, val size: 100\n",
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 10/300 \t lr=3.3e-04\t t=2s \tloss=0.9941\tval_loss=0.9951\tcorr=0.2911\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=4s \tloss=0.9425\tval_loss=0.9430\tcorr=0.4362\n",
      "Epoch 30/300 \t lr=1.0e-03\t t=2s \tloss=0.5697\tval_loss=0.5738\tcorr=0.4758\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=3s \tloss=0.4278\tval_loss=0.4109\tcorr=0.6145\n",
      "Epoch 50/300 \t lr=9.3e-04\t t=2s \tloss=0.3434\tval_loss=0.3376\tcorr=0.6785\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=1s \tloss=0.2914\tval_loss=0.2981\tcorr=0.7106\n",
      "Epoch 70/300 \t lr=8.5e-04\t t=2s \tloss=0.2553\tval_loss=0.2662\tcorr=0.7425\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=2s \tloss=0.2302\tval_loss=0.2455\tcorr=0.7611\n",
      "Epoch 90/300 \t lr=7.8e-04\t t=3s \tloss=0.2096\tval_loss=0.2293\tcorr=0.7765\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=2s \tloss=0.1970\tval_loss=0.2220\tcorr=0.7829\n",
      "Epoch 110/300 \t lr=7.0e-04\t t=2s \tloss=0.1888\tval_loss=0.2167\tcorr=0.7871\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=2s \tloss=0.1845\tval_loss=0.2175\tcorr=0.7862\n",
      "Epoch 130/300 \t lr=6.3e-04\t t=2s \tloss=0.1806\tval_loss=0.2175\tcorr=0.7848\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=2s \tloss=0.1791\tval_loss=0.2197\tcorr=0.7822\n",
      "Epoch 150/300 \t lr=5.6e-04\t t=2s \tloss=0.1738\tval_loss=0.2197\tcorr=0.7826\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=2s \tloss=0.1699\tval_loss=0.2249\tcorr=0.7765\n",
      "Epoch 170/300 \t lr=4.8e-04\t t=2s \tloss=0.1634\tval_loss=0.2255\tcorr=0.7760\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=2s \tloss=0.1573\tval_loss=0.2266\tcorr=0.7746\n",
      "Epoch 190/300 \t lr=4.1e-04\t t=2s \tloss=0.1519\tval_loss=0.2297\tcorr=0.7718\n",
      "Epoch 200/300 \t lr=3.7e-04\t t=2s \tloss=0.1468\tval_loss=0.2298\tcorr=0.7722\n",
      "Epoch 210/300 \t lr=3.3e-04\t t=2s \tloss=0.1404\tval_loss=0.2328\tcorr=0.7676\n",
      "Epoch 220/300 \t lr=3.0e-04\t t=2s \tloss=0.1339\tval_loss=0.2359\tcorr=0.7656\n",
      "Epoch 230/300 \t lr=2.6e-04\t t=2s \tloss=0.1295\tval_loss=0.2343\tcorr=0.7669\n",
      "Epoch 240/300 \t lr=2.2e-04\t t=2s \tloss=0.1240\tval_loss=0.2357\tcorr=0.7653\n",
      "Epoch 250/300 \t lr=1.9e-04\t t=2s \tloss=0.1194\tval_loss=0.2377\tcorr=0.7631\n",
      "Epoch 260/300 \t lr=1.5e-04\t t=2s \tloss=0.1155\tval_loss=0.2380\tcorr=0.7633\n",
      "Epoch 270/300 \t lr=1.1e-04\t t=2s \tloss=0.1122\tval_loss=0.2382\tcorr=0.7628\n",
      "Epoch 280/300 \t lr=7.4e-05\t t=2s \tloss=0.1098\tval_loss=0.2387\tcorr=0.7621\n",
      "Epoch 290/300 \t lr=3.7e-05\t t=2s \tloss=0.1081\tval_loss=0.2391\tcorr=0.7617\n",
      "Epoch 300/300 \t lr=0.0e+00\t t=1s \tloss=0.1075\tval_loss=0.2393\tcorr=0.7622\n",
      "[0.3078787878787879, 0.8472727272727274, 0.9010101010101011, 0.964848484848485, 0.9159595959595961, 0.7232323232323233, 0.9236363636363638, 0.7975757575757577] \n",
      "\n",
      "\n",
      "FOLD 3\n",
      "train size: 400, val size: 100\n",
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 10/300 \t lr=3.3e-04\t t=3s \tloss=0.9944\tval_loss=0.9943\tcorr=0.3431\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=4s \tloss=0.9478\tval_loss=0.9405\tcorr=0.5139\n",
      "Epoch 30/300 \t lr=1.0e-03\t t=2s \tloss=0.5557\tval_loss=0.6237\tcorr=0.3765\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=2s \tloss=0.4375\tval_loss=0.4672\tcorr=0.5449\n",
      "Epoch 50/300 \t lr=9.3e-04\t t=3s \tloss=0.3539\tval_loss=0.4032\tcorr=0.6156\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=2s \tloss=0.2950\tval_loss=0.3554\tcorr=0.6555\n",
      "Epoch 70/300 \t lr=8.5e-04\t t=3s \tloss=0.2612\tval_loss=0.3254\tcorr=0.6808\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=2s \tloss=0.2372\tval_loss=0.3075\tcorr=0.6997\n",
      "Epoch 90/300 \t lr=7.8e-04\t t=2s \tloss=0.2196\tval_loss=0.2888\tcorr=0.7165\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=2s \tloss=0.2037\tval_loss=0.2656\tcorr=0.7390\n",
      "Epoch 110/300 \t lr=7.0e-04\t t=2s \tloss=0.1898\tval_loss=0.2486\tcorr=0.7575\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=1s \tloss=0.1817\tval_loss=0.2417\tcorr=0.7623\n",
      "Epoch 130/300 \t lr=6.3e-04\t t=3s \tloss=0.1754\tval_loss=0.2396\tcorr=0.7633\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=2s \tloss=0.1695\tval_loss=0.2397\tcorr=0.7620\n",
      "Epoch 150/300 \t lr=5.6e-04\t t=2s \tloss=0.1631\tval_loss=0.2399\tcorr=0.7624\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=2s \tloss=0.1553\tval_loss=0.2412\tcorr=0.7609\n",
      "Epoch 170/300 \t lr=4.8e-04\t t=2s \tloss=0.1488\tval_loss=0.2421\tcorr=0.7588\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=2s \tloss=0.1369\tval_loss=0.2460\tcorr=0.7561\n",
      "Epoch 190/300 \t lr=4.1e-04\t t=2s \tloss=0.1301\tval_loss=0.2495\tcorr=0.7528\n",
      "Epoch 200/300 \t lr=3.7e-04\t t=2s \tloss=0.1216\tval_loss=0.2438\tcorr=0.7574\n",
      "Epoch 210/300 \t lr=3.3e-04\t t=2s \tloss=0.1126\tval_loss=0.2452\tcorr=0.7558\n",
      "Epoch 220/300 \t lr=3.0e-04\t t=2s \tloss=0.1059\tval_loss=0.2464\tcorr=0.7549\n",
      "Epoch 230/300 \t lr=2.6e-04\t t=2s \tloss=0.1012\tval_loss=0.2486\tcorr=0.7515\n",
      "Epoch 240/300 \t lr=2.2e-04\t t=2s \tloss=0.0955\tval_loss=0.2494\tcorr=0.7514\n",
      "Epoch 250/300 \t lr=1.9e-04\t t=2s \tloss=0.0903\tval_loss=0.2486\tcorr=0.7525\n",
      "Epoch 260/300 \t lr=1.5e-04\t t=2s \tloss=0.0865\tval_loss=0.2480\tcorr=0.7535\n",
      "Epoch 270/300 \t lr=1.1e-04\t t=2s \tloss=0.0831\tval_loss=0.2488\tcorr=0.7524\n",
      "Epoch 280/300 \t lr=7.4e-05\t t=2s \tloss=0.0806\tval_loss=0.2500\tcorr=0.7508\n",
      "Epoch 290/300 \t lr=3.7e-05\t t=2s \tloss=0.0790\tval_loss=0.2504\tcorr=0.7506\n",
      "Epoch 300/300 \t lr=0.0e+00\t t=2s \tloss=0.0784\tval_loss=0.2508\tcorr=0.7502\n",
      "[0.26343434343434347, 0.884848484848485, 0.8892929292929294, 0.9644444444444447, 0.890909090909091, 0.593939393939394, 0.903838383838384, 0.7915151515151516] \n",
      "\n",
      "\n",
      "FOLD 4\n",
      "train size: 400, val size: 100\n",
      "Model parameters count: 47929357\n",
      "train batch num: 1\n",
      "val batch num: 1\n",
      "optim: Adam, lr: 0.001, warmup_steps: 30\n",
      "Epoch 10/300 \t lr=3.3e-04\t t=3s \tloss=0.9943\tval_loss=0.9939\tcorr=0.3554\n",
      "Epoch 20/300 \t lr=6.7e-04\t t=2s \tloss=0.9450\tval_loss=0.9337\tcorr=0.5328\n",
      "Epoch 30/300 \t lr=1.0e-03\t t=2s \tloss=0.5504\tval_loss=0.5294\tcorr=0.5242\n",
      "Epoch 40/300 \t lr=9.6e-04\t t=3s \tloss=0.4297\tval_loss=0.4349\tcorr=0.5714\n",
      "Epoch 50/300 \t lr=9.3e-04\t t=4s \tloss=0.3548\tval_loss=0.3543\tcorr=0.6593\n",
      "Epoch 60/300 \t lr=8.9e-04\t t=3s \tloss=0.2945\tval_loss=0.2909\tcorr=0.7227\n",
      "Epoch 70/300 \t lr=8.5e-04\t t=3s \tloss=0.2574\tval_loss=0.2677\tcorr=0.7415\n",
      "Epoch 80/300 \t lr=8.1e-04\t t=2s \tloss=0.2283\tval_loss=0.2480\tcorr=0.7599\n",
      "Epoch 90/300 \t lr=7.8e-04\t t=2s \tloss=0.2062\tval_loss=0.2384\tcorr=0.7668\n",
      "Epoch 100/300 \t lr=7.4e-04\t t=3s \tloss=0.1914\tval_loss=0.2280\tcorr=0.7771\n",
      "Epoch 110/300 \t lr=7.0e-04\t t=2s \tloss=0.1852\tval_loss=0.2200\tcorr=0.7844\n",
      "Epoch 120/300 \t lr=6.7e-04\t t=2s \tloss=0.1810\tval_loss=0.2180\tcorr=0.7847\n",
      "Epoch 130/300 \t lr=6.3e-04\t t=2s \tloss=0.1751\tval_loss=0.2174\tcorr=0.7838\n",
      "Epoch 140/300 \t lr=5.9e-04\t t=2s \tloss=0.1689\tval_loss=0.2181\tcorr=0.7835\n",
      "Epoch 150/300 \t lr=5.6e-04\t t=2s \tloss=0.1675\tval_loss=0.2184\tcorr=0.7827\n",
      "Epoch 160/300 \t lr=5.2e-04\t t=2s \tloss=0.1598\tval_loss=0.2125\tcorr=0.7888\n",
      "Epoch 170/300 \t lr=4.8e-04\t t=2s \tloss=0.1540\tval_loss=0.2168\tcorr=0.7848\n",
      "Epoch 180/300 \t lr=4.4e-04\t t=2s \tloss=0.1471\tval_loss=0.2125\tcorr=0.7895\n",
      "Epoch 190/300 \t lr=4.1e-04\t t=2s \tloss=0.1404\tval_loss=0.2143\tcorr=0.7869\n",
      "Epoch 200/300 \t lr=3.7e-04\t t=2s \tloss=0.1316\tval_loss=0.2116\tcorr=0.7890\n",
      "Epoch 210/300 \t lr=3.3e-04\t t=2s \tloss=0.1255\tval_loss=0.2137\tcorr=0.7873\n",
      "Epoch 220/300 \t lr=3.0e-04\t t=2s \tloss=0.1172\tval_loss=0.2158\tcorr=0.7866\n",
      "Epoch 230/300 \t lr=2.6e-04\t t=2s \tloss=0.1107\tval_loss=0.2161\tcorr=0.7859\n",
      "Epoch 240/300 \t lr=2.2e-04\t t=2s \tloss=0.1038\tval_loss=0.2156\tcorr=0.7862\n",
      "Epoch 250/300 \t lr=1.9e-04\t t=2s \tloss=0.0986\tval_loss=0.2151\tcorr=0.7862\n",
      "Epoch 260/300 \t lr=1.5e-04\t t=2s \tloss=0.0949\tval_loss=0.2163\tcorr=0.7850\n",
      "Epoch 270/300 \t lr=1.1e-04\t t=2s \tloss=0.0904\tval_loss=0.2145\tcorr=0.7863\n",
      "Epoch 280/300 \t lr=7.4e-05\t t=2s \tloss=0.0877\tval_loss=0.2151\tcorr=0.7862\n",
      "Epoch 290/300 \t lr=3.7e-05\t t=2s \tloss=0.0860\tval_loss=0.2152\tcorr=0.7859\n",
      "Epoch 300/300 \t lr=0.0e+00\t t=2s \tloss=0.0854\tval_loss=0.2154\tcorr=0.7857\n",
      "[0.3393939393939394, 0.8525252525252527, 0.9179797979797981, 0.9636363636363638, 0.9131313131313133, 0.711919191919192, 0.9357575757575758, 0.8024242424242426] \n",
      "\n",
      "\n",
      "0.79286 [0.3078, 0.87152, 0.90384, 0.96145, 0.90044, 0.67943, 0.91846, 0.79992]\n"
     ]
    }
   ],
   "source": [
    "loss_fct = CVPRLoss_tanh1()\n",
    "# loss_fct = CVPRLoss_pair()\n",
    "k=5\n",
    "scoref = []\n",
    "skf = KFold(n_splits=k, shuffle=False)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train_df)):   \n",
    "    print(f'FOLD {index}')\n",
    "    train0 = train_df.iloc[train_index]\n",
    "    val0 = train_df.iloc[test_index]   \n",
    "    train_dataset = MyDataset(train0, use_cols, target_cols)\n",
    "    val_dataset = MyDataset(val0, use_cols, target_cols)\n",
    "    print(f'train size: {len(train0)}, val size: {len(val0)}')\n",
    "\n",
    "    modelname = f'lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe'\n",
    "    seed_everything(seed)\n",
    "    model = CVPRModel(input_dim=93,\n",
    "                    num_classes=8,\n",
    "                    bi=True,\n",
    "                    time_step=37\n",
    "                   ).to(device)\n",
    "\n",
    "    _ = train_sig(model, \n",
    "                train_dataset, \n",
    "                val_dataset, \n",
    "                verbose=10, \n",
    "                fold_=index,\n",
    "                modelname=modelname,\n",
    "                modelpath=modelpath,\n",
    "                input='input',\n",
    "                y='y',\n",
    "                debug=False\n",
    "                 )\n",
    "    scoref.append(_)\n",
    "scoreff = scoref\n",
    "print(np.round(np.array(scoreff).mean(1).mean(), 5), [round(x, 5) for x in np.array(scoreff).mean(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.79236 [0.31685, 0.87071, 0.90166, 0.96186, 0.89802, 0.67467, 0.91814, 0.79701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7803,\n",
       " [0.26618, 0.86804, 0.89471, 0.95782, 0.89059, 0.66764, 0.90909, 0.78836])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.7 0.77966 [0.26691, 0.86489, 0.8943, 0.96145, 0.89042, 0.65576, 0.912, 0.79152]\n",
    "(0.7803, [0.26618, 0.86804, 0.89471, 0.95782, 0.89059, 0.66764, 0.90909, 0.78836])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fct = CVPRLoss_softsign()\n",
    "# k=5\n",
    "# scoref = []\n",
    "# skf = KFold(n_splits=k, shuffle=False)\n",
    "# for index, (train_index, test_index) in enumerate(skf.split(train_df)):   \n",
    "#     print(f'FOLD {index}')\n",
    "#     train0 = train_df.iloc[train_index]\n",
    "#     val0 = train_df.iloc[test_index]   \n",
    "#     train_dataset = MyDataset(train0, use_cols, target_cols)\n",
    "#     val_dataset = MyDataset(val0, use_cols, target_cols)\n",
    "#     print(f'train size: {len(train0)}, val size: {len(val0)}')\n",
    "\n",
    "#     modelname = f'LSTM_2layer_step37_kendall_8all_softsign'\n",
    "#     seed_everything(seed)\n",
    "#     model = CVPRModel(input_dim=1,\n",
    "#                     num_classes=8,\n",
    "#                     bi=True,\n",
    "#                     time_step=37\n",
    "#                    ).to(device)\n",
    "#     preds,_ = train(model, \n",
    "#                 train_dataset, \n",
    "#                 val_dataset, \n",
    "#                 verbose=100, \n",
    "#                 fold_=index,\n",
    "#                 modelname=modelname,\n",
    "#                 modelpath=modelpath,\n",
    "#                 input='input',\n",
    "#                 y='y',\n",
    "#                 debug=False\n",
    "#                  )\n",
    "#     scoref.append(_)\n",
    "# scoreff = scoref\n",
    "# np.round(np.array(scoreff).mean(1).mean(), 5), [round(x, 5) for x in np.array(scoreff).mean(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79286,\n",
       " [0.3078, 0.87152, 0.90384, 0.96145, 0.90044, 0.67943, 0.91846, 0.79992])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array(scoreff).mean(1).mean(), 5), [round(x, 5) for x in np.array(scoreff).mean(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0.78795,\n",
    "#  [0.26836, 0.87111, 0.90545, 0.96509, 0.89745, 0.67507, 0.91895, 0.8021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cplfw_rank': 0,\n",
       " 'market1501_rank': 0,\n",
       " 'dukemtmc_rank': 0,\n",
       " 'msmt17_rank': 0,\n",
       " 'veri_rank': 0,\n",
       " 'vehicleid_rank': 0,\n",
       " 'veriwild_rank': 0,\n",
       " 'sop_rank': 0,\n",
       " 'arch': 'j121221121221221311331321121221000000'}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载test\n",
    "with open('./data/CVPR_2022_NAS_Track2_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_data['arch99997']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_df(test_data)\n",
    "get_step(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_trans(test_df, emb_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>...</th>\n",
       "      <th>veriwild_rank</th>\n",
       "      <th>sop_rank</th>\n",
       "      <th>id</th>\n",
       "      <th>arch</th>\n",
       "      <th>head</th>\n",
       "      <th>mlp</th>\n",
       "      <th>emb</th>\n",
       "      <th>depth</th>\n",
       "      <th>all_emb</th>\n",
       "      <th>all_emb_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>arch501</td>\n",
       "      <td>k231131211331131121131121311111211000</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3, 2.0, 3.0, 1.0, 1.0, 3.0, 1.0, 2.0, 1.0, 1....</td>\n",
       "      <td>[2, 4, 8, 9, 10, 15, 16, 18, 20, 23, 26, 29, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>arch502</td>\n",
       "      <td>l111211211121121111331311221211121111</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1....</td>\n",
       "      <td>[0, 3, 6, 9, 11, 13, 16, 18, 20, 23, 24, 28, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>arch503</td>\n",
       "      <td>j211321131221321331211131321131000000</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2, 2.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 3.0, 1....</td>\n",
       "      <td>[1, 4, 6, 9, 12, 14, 16, 17, 22, 23, 25, 28, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   col0  col1  col2  col3  col4  col5  col6  col7  col8  col9  ...  \\\n",
       "0   3.0   2.0   3.0   1.0   1.0   3.0   1.0   2.0   1.0   1.0  ...   \n",
       "1   1.0   1.0   1.0   1.0   2.0   1.0   1.0   2.0   1.0   1.0  ...   \n",
       "2   2.0   2.0   1.0   1.0   3.0   2.0   1.0   1.0   3.0   1.0  ...   \n",
       "\n",
       "   veriwild_rank  sop_rank       id                                   arch  \\\n",
       "0            0.0       0.0  arch501  k231131211331131121131121311111211000   \n",
       "1            0.0       0.0  arch502  l111211211121121111331311221211121111   \n",
       "2            0.0       0.0  arch503  j211321131221321331211131321131000000   \n",
       "\n",
       "                                                head  \\\n",
       "0  [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "1  [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "2  [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...   \n",
       "\n",
       "                                                 mlp  \\\n",
       "0  [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "1  [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "2  [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n",
       "\n",
       "                                                 emb  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
       "1  [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
       "2  [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                               depth  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             all_emb  \\\n",
       "0  [3, 2.0, 3.0, 1.0, 1.0, 3.0, 1.0, 2.0, 1.0, 1....   \n",
       "1  [1, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1....   \n",
       "2  [2, 2.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 3.0, 1....   \n",
       "\n",
       "                                         all_emb_vec  \n",
       "0  [2, 4, 8, 9, 10, 15, 16, 18, 20, 23, 26, 29, 3...  \n",
       "1  [0, 3, 6, 9, 11, 13, 16, 18, 20, 23, 24, 28, 3...  \n",
       "2  [1, 4, 6, 9, 12, 14, 16, 17, 22, 23, 25, 28, 3...  \n",
       "\n",
       "[3 rows x 53 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_cuda(test_df, model):\n",
    "    #获得预测\n",
    "    test_dataset = MyDataset(test_df, use_cols, target_cols, 0)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1024*16,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            pred = model(data['input'].to(device))\n",
    "            preds.append(pred.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    print(preds.shape)\n",
    "\n",
    "    del test_dataset, test_loader, model;\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return preds\n",
    "        \n",
    "def predict_sig(test_df, k=5, modelname=''):\n",
    "    for target in range(8):        \n",
    "        cols = []\n",
    "        for fold_ in range(k):\n",
    "            modelname1 = f'{modelname}_target{target}_{fold_}.pt'\n",
    "            print(f'Model {modelname1}')\n",
    "            model = CVPRModel(input_dim=93,\n",
    "                        num_classes=8,\n",
    "                        bi=True,\n",
    "                        time_step=37\n",
    "                       ).to(device)\n",
    "            model.load_state_dict(torch.load(modelpath+modelname1))\n",
    "\n",
    "            pred_ = pred_cuda(test_df, model)[:, target]\n",
    "            tmp_c = f'{target_cols[target]}_{fold_}'\n",
    "            test_df[tmp_c] = pred_\n",
    "            cols.append(tmp_c)\n",
    "            print(f'Done {tmp_c}')\n",
    "            \n",
    "        print(cols)\n",
    "        test_df[cols] = test_df[cols].rank()\n",
    "        test_df[target_cols[target]] = test_df[cols].mean(axis=1).rank()\n",
    "        \n",
    "def predict_all(test_df, k=5, modelname = f'lstm2y稠密_bc400_lr001_softsign'):\n",
    "\n",
    "    print(f'Model {modelname}')\n",
    "\n",
    "    cols = []\n",
    "    for fold_ in range(k):\n",
    "        model = CVPRModel(input_dim=1,\n",
    "                    num_classes=8,\n",
    "                    bi=True,\n",
    "                    time_step=37\n",
    "                   ).to(device)\n",
    "        model.load_state_dict(torch.load(modelpath+f'{modelname}_{fold_}.pt'))\n",
    "\n",
    "        pred_ = pred_cuda(test_df, model)\n",
    "        tmp_c = [f'{target}_{fold_}' for target in target_cols]\n",
    "        test_df[tmp_c] = pred_\n",
    "        cols += tmp_c\n",
    "    #get rank\n",
    "    print(cols)\n",
    "    test_df[cols] = test_df[cols].rank()\n",
    "    for c in target_cols:\n",
    "        test_df[c] = test_df[[f'{c}_{fold_}' for fold_ in range(k)]].mean(axis=1)\n",
    "        test_df[c] = test_df[c].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 'lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_2layer_step37_kendall_8all_softrankloss\n",
    "# LSTM_2layer_step37_kendall_8all_softsign\n",
    "# LSTM_2layer_step37_kendall_8all_tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df1 = train_df.copy()\n",
    "# predict_sig(train_df1, k=5, modelname='lstm2y稠密_bc400_lr001_tanh_mat')\n",
    "# compute_metric(train_df[target_cols].values, train_df1[target_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[x+'_trans_y' for x in target_cols]]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82284192"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.37461616, 0.90014512, 0.91819605, 0.97460526, 0.92847466,\n",
    "       0.72192225, 0.92732675, 0.83744911])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target0_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\45928\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771ce8b8524f40cba6c34003a9c718dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done cplfw_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target0_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd49546d1ca741b08da3676641704724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done cplfw_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target0_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20735e0ab7fa4300bd2112902fd8287e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done cplfw_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target0_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b121aa3dcc846fba5909de29156182d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done cplfw_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target0_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b2bda3f4ec41e285b0fd478ed35331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done cplfw_rank_4\n",
      "['cplfw_rank_0', 'cplfw_rank_1', 'cplfw_rank_2', 'cplfw_rank_3', 'cplfw_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target1_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c282edce1d40669f0346760e72d4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done market1501_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target1_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288a313e1699415e8aba3e18a7b5d98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done market1501_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target1_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f531a20996e94a69a7fee7b4b0aea977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done market1501_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target1_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a648214b2441e99849be96c5f58827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done market1501_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target1_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6dd2d1254c4592b7bc6721e51f9abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done market1501_rank_4\n",
      "['market1501_rank_0', 'market1501_rank_1', 'market1501_rank_2', 'market1501_rank_3', 'market1501_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target2_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae7069bae0541a58de963136db80a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done dukemtmc_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target2_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3891ae9168433ab2eb73747b4fe0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done dukemtmc_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target2_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d501e5e2e2214c6b95b7823974f71370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done dukemtmc_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target2_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60b44bd22fd43fbbb499da2094f642f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done dukemtmc_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target2_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078243134bfe445180b80fb4275d0f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done dukemtmc_rank_4\n",
      "['dukemtmc_rank_0', 'dukemtmc_rank_1', 'dukemtmc_rank_2', 'dukemtmc_rank_3', 'dukemtmc_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target3_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058f81733e294649bed71cb7584ff992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done msmt17_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target3_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a518ef42807434e84997f92797d7212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done msmt17_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target3_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34369bfbd9a4446b521d575500b9d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done msmt17_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target3_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c9ecea6d884813b085a917385a6722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done msmt17_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target3_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ed24b34dac40b59febc54e03a9adad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done msmt17_rank_4\n",
      "['msmt17_rank_0', 'msmt17_rank_1', 'msmt17_rank_2', 'msmt17_rank_3', 'msmt17_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target4_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2270ae8c1b844ccd8b36c558f1ecf90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veri_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target4_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb4b787cf024418a9a4e0b639706e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veri_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target4_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16265c0d7277493fad4e843e601475d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veri_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target4_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6e85f6aca546409a2df44116c4502d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veri_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target4_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f569667009a6472ca900507cf754a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veri_rank_4\n",
      "['veri_rank_0', 'veri_rank_1', 'veri_rank_2', 'veri_rank_3', 'veri_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target5_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdea71473b7645fa94c66b5464cf52ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done vehicleid_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target5_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76937155b69b40a5ab5e43e9ccb2a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done vehicleid_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target5_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42aceaabf18040219b061858f7eaf545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done vehicleid_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target5_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba854e529864f5388434e63d3abf87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done vehicleid_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target5_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b4d8d761f437cbe242995e3104b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done vehicleid_rank_4\n",
      "['vehicleid_rank_0', 'vehicleid_rank_1', 'vehicleid_rank_2', 'vehicleid_rank_3', 'vehicleid_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target6_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ae51e64ba84e849e29071855c7a8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veriwild_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target6_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2828a3485764ebfa13a6915c68a8f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veriwild_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target6_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d35e8f46e24fc7b0c8f25a14d1934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veriwild_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target6_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f70f3a0f5104572ae7743625929399e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veriwild_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target6_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3498228f2f4102856122eb174f6732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done veriwild_rank_4\n",
      "['veriwild_rank_0', 'veriwild_rank_1', 'veriwild_rank_2', 'veriwild_rank_3', 'veriwild_rank_4']\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target7_0.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce05b068a6b4e33a9199f41b961fb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done sop_rank_0\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target7_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448d476d8203430bb7aaa96626569f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done sop_rank_1\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target7_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd93ef713e74602b0bc60ddf406d4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done sop_rank_2\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target7_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd545db4b224b088aa4b7a09b2847b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done sop_rank_3\n",
      "Model lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_target7_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d32fdbfc9644e309a5f34f5995325c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99500, 8)\n",
      "Done sop_rank_4\n",
      "['sop_rank_0', 'sop_rank_1', 'sop_rank_2', 'sop_rank_3', 'sop_rank_4']\n"
     ]
    }
   ],
   "source": [
    "predict_sig(test_df, k=5, modelname='lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_all(test_df, k=5, modelname='lstm2y稠密_bc400_lr001_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[target_cols] = test_df[target_cols].astype(int)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cplfw_rank         91761\n",
       "market1501_rank    91358\n",
       "dukemtmc_rank      92086\n",
       "msmt17_rank        92015\n",
       "veri_rank          92005\n",
       "vehicleid_rank     93083\n",
       "veriwild_rank      92253\n",
       "sop_rank           91416\n",
       "dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[target_cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_20220516'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_20220516'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\45928\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8607ba7fb6b44a3a57a73c409274175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#输出\n",
    "for i in tqdm(test_df[['id']+target_cols].values):\n",
    "    id_ = i[0]\n",
    "    for k,v in enumerate(target_cols):\n",
    "        k += 1\n",
    "        test_data[id_][v] = i[k]\n",
    "        \n",
    "with open(f'./sub/{modelname}.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstm2y_加权_bc400_shuffle_lr001_tanh1_ohe_20220516'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
